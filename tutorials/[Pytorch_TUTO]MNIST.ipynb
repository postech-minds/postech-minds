{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [`PyTorch` Tutorial] Introduction to Deep Learning\n",
    "\n",
    "<img width = '1000' height = '600' src = 'https://github.com/SHlee-TDA/Machine-Learning/blob/main/1.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Artificial Neural Networks (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Motivation\n",
    "\n",
    "<img width = '1000' height = '600' src = 'https://github.com/SHlee-TDA/Machine-Learning/blob/main/2.png?raw=true'>\n",
    "\n",
    "Let's begin with simplest examples.\n",
    "\n",
    "Consider two binary classification problems.\n",
    "\n",
    "It is very typical situation in machine learning. \n",
    "\n",
    "One might say that what we should do to solve this problem is to *__find an appropriate decision boundary function__*.\n",
    "\n",
    "In the left figure, *linear* decision boundary functions can split a plane into two regions for each class.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width = '600' height = '400' src = 'https://github.com/SHlee-TDA/Machine-Learning/blob/main/3.png?raw=true'>\n",
    "<img width = '600' height = '400' src = 'https://github.com/SHlee-TDA/Machine-Learning/blob/main/4.png?raw=true'>\n",
    "\n",
    "However, in the right figure, two classes are __never__ split by linear function.\n",
    "\n",
    "Hence, we need to find *__non-linear__* decision boundary function.\n",
    "\n",
    "\n",
    "\n",
    "<img width = '1000' height = '600' src = 'https://github.com/SHlee-TDA/Machine-Learning/blob/main/5.png?raw=true'>\n",
    "\n",
    "\n",
    "\n",
    "**Multi-Layer Perceptron(MLP)** we will study now is one way to generate a *__non-linear decision boundary function__*.\n",
    "\n",
    "And network structures based on MLP is called *__Artificial Neural Network__*.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Perceptron\n",
    "\n",
    "<img width = '1000' height = '600' src = 'https://github.com/SHlee-TDA/Machine-Learning/blob/main/6.png?raw=true'>\n",
    "\n",
    "Neural Networks consists of neurons as basic units\n",
    "\n",
    "Neuron receives input signals and sends this signal to the next neuron when the signal are over a certain threshold.\n",
    "\n",
    "(Artificial) Perceptron is the algorithmic implement to neuron cell.\n",
    "\n",
    "It receives input signals as a vector or a tensor and transforms them linearly.\n",
    "\n",
    "And after filtering the signal according to certain threshold, it sends the signals to the next perceptron.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Activation function\n",
    "\n",
    "\n",
    "<img width = '1000' height = '600' src = 'https://github.com/SHlee-TDA/Machine-Learning/blob/main/7.png?raw=true'>\n",
    "\n",
    "It is the *__activation function__* that plays the role of this threshold criteria.\n",
    "\n",
    "There are several activation functions: __Step function__, __Sigmoid function__, __ReLU(Rectified Linear Unit)__.\n",
    "\n",
    "The single pipeline from linear transform to activation function is called *Single-Layer Perceptron(SLP)*.\n",
    "\n",
    "It is well known that the single-layer perceptron works well for linear classification problem.\n",
    "\n",
    "\n",
    "Refer to [*Perceptron Convergence Theorem*](https://en.wikipedia.org/wiki/Perceptron#Convergence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation : Single-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import pytorch tools\n",
    "\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pytorch uses 'tensor' type data as its unit.\n",
    "## Refer to the following link if you want to know why tensor is better than list or numpy:\n",
    "## https://hiddenbeginner.github.io/deeplearning/2020/01/21/pytorch_tensor.html\n",
    "\n",
    "data = [1.0, 2.0, 3.0]\n",
    "X = torch.Tensor(data)\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9055, -0.6772,  0.6023], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pytorch 'Linear' layer works a linear transform just like the name.\n",
    "\n",
    "linear = nn.Linear(in_features = 3, out_features = 3)   # nn.Linear(3,3) makes randomly chosen 3x3 matrix.\n",
    "linear(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.6023], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Activation function works 'thresholding' to processed signal.\n",
    "\n",
    "step = nn.ReLU()    # 'Rectified Linear Unit(ReLU(x) = max(0,x)) is a famous activation function.\n",
    "step(linear(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.6023], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## nn.Sequential binds materials for your perceptron layer. \n",
    "\n",
    "Perceptron = nn.Sequential(linear, step)    # Our single perceptron layer.\n",
    "Perceptron(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Multi-Layer Perceptron (MLP)\n",
    "\n",
    "<img width = '1000' height = '600' src = 'https://github.com/SHlee-TDA/Machine-Learning/blob/main/8.png?raw=true'>\n",
    "\n",
    "*__Multi-Layer Perceptron(MLP)__* is nothing but a sequence of several perceptron.\n",
    "\n",
    "But, literally, \"Simple is really Best!\"\n",
    "\n",
    "Stacking linear transforms and activation functions makes non-linearity.\n",
    "\n",
    "And this can generate very powerful non-linear decision boundary function after training. \n",
    "\n",
    "Refer to [*Universal Approximation Theorem*](https://en.wikipedia.org/wiki/Universal_approximation_theorem)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation : MNIST and MLP\n",
    "\n",
    "MNIST data is 'Hello world!' example in Machine Learning field.\n",
    "\n",
    "MNIST consists of total 60,000 hand-written digits images for training data with labels and 10,000 images for test data.\n",
    "\n",
    "Now let's make a digits classifier based on MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "# Download MNIST data sets and load them as Tensor form.\n",
    "\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image is  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Choose a sample image.\n",
    "\n",
    "sample_img, sample_label = mnist_train[0]\n",
    "plt.matshow(sample_img[0])\n",
    "print('This image is ', sample_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Image data is of the form 28*28.\n",
    "## We need to flatten this image to put in linear layer.\n",
    "\n",
    "flat = nn.Flatten()\n",
    "flat(sample_img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple two-layer perceptron network.\n",
    "\n",
    "class ANN(nn.Module):                   ## \n",
    "    def __init__(self):\n",
    "        super(ANN, self).__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    # 'forward' sets the data in the direction of progress.\n",
    "    # This will use for calculating gradient later.\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (layer1): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (out): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Define our model.\n",
    "\n",
    "model = ANN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0111,  0.0352,  0.0130, -0.0305,  0.0312, -0.0153, -0.0136,  0.0370,\n",
       "         -0.0052, -0.0448]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Put in the sample image to our model.\n",
    "\n",
    "model(sample_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The argmax value of model output is the prediction of model for our sample image.\n",
    "## Maybe your prediction is not correct. \n",
    "## In fact, there is no reason for model to predict correctly.\n",
    "\n",
    "model(sample_img).argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Gradient Descent\n",
    "\n",
    "\n",
    "\n",
    "<img width = '1000' height = '600' src = 'https://github.com/SHlee-TDA/Machine-Learning/blob/main/9.png?raw=true'>\n",
    "\n",
    "After we get a prediction $\\tilde{y}$, the prediction error is obtained by calculating the *loss function* between the prediction $\\tilde{y}$ and the label $y$.\n",
    "\n",
    "Gradient descent is an algorithm in order to update the weights $W$ and reduce the loss function value.\n",
    "\n",
    "In doing so, neural network models are trained and then generate more improved decision function.\n",
    "\n",
    "For more details about gradient descent and several optimization algorithms, see the following [link](https://hiddenbeginner.github.io/deeplearning/2019/09/22/optimization_algorithms_in_deep_learning.html).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation : Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the training parameters\n",
    "lr = 0.001              # learning rate\n",
    "training_epochs = 15    # epoch numbers\n",
    "batch_size = 512        # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader sets the overall training process of the model.  \n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose loss function (criterion) and optimization algorithm (optimizer).\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()    \n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.453651726\n",
      "Epoch: 0002 cost = 0.163055867\n",
      "Epoch: 0003 cost = 0.107910745\n",
      "Epoch: 0004 cost = 0.075972177\n",
      "Epoch: 0005 cost = 0.055778354\n",
      "Epoch: 0006 cost = 0.044476148\n",
      "Epoch: 0007 cost = 0.034060862\n",
      "Epoch: 0008 cost = 0.026254028\n",
      "Epoch: 0009 cost = 0.020116607\n",
      "Epoch: 0010 cost = 0.015809579\n",
      "Epoch: 0011 cost = 0.012971240\n",
      "Epoch: 0012 cost = 0.009805708\n",
      "Epoch: 0013 cost = 0.008101291\n",
      "Epoch: 0014 cost = 0.006497889\n",
      "Epoch: 0015 cost = 0.005705670\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "## Training Algorithm\n",
    "\n",
    "total_batch = len(data_loader)\n",
    "\n",
    "for epoch in range(training_epochs):    # Training is performed as much as epoch.\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X, Y in data_loader:            # X is input image, Y is label of the image\n",
    "\n",
    "        optimizer.zero_grad()           # Initialize optimizer to zero.\n",
    "        prediction = model(X)           # Calculate prediction.\n",
    "        \n",
    "        cost = criterion(prediction, Y) # Cost is calculated \n",
    "        cost.backward()                 # Calculate gradients by back-propagation.\n",
    "        optimizer.step()                # Do optimization step\n",
    "\n",
    "        avg_cost += cost / total_batch  # Record the value of cost.\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "## Test our model works well. \n",
    "\n",
    "Y = model(sample_img)\n",
    "pred = Y.argmax()\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9800999760627747\n"
     ]
    }
   ],
   "source": [
    "# Test the model using test sets\n",
    "\n",
    "with torch.no_grad(): # In order to test your model, YOU NEED TO SET `with torch.no_grad()`!!\n",
    "    X_test = mnist_test.data.float()\n",
    "    Y_test = mnist_test.targets\n",
    "\n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Artificial Neural Network (ANN)\n",
    "\n",
    "Are you satisfied with the accuracy?\n",
    "\n",
    "This framework is called Artificial Neural Network (ANN).\n",
    "\n",
    "\n",
    "<img width = '1000' height = '600' src = 'https://github.com/SHlee-TDA/Machine-Learning/blob/main/10.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Motivation\n",
    "\n",
    "ANN was good. \n",
    "\n",
    "However, note that ANN need to flatten image to input your data.\n",
    "\n",
    "Thus *the shape of data* might forget.\n",
    "\n",
    "CNN is one of method to consider the shape of data.\n",
    "\n",
    "\n",
    "<img width = '1000' height = '600' src ='https://github.com/SHlee-TDA/Machine-Learning/blob/main/11.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Convolution \n",
    "\n",
    "<img width = '1000' height = '600' src = 'https://github.com/SHlee-TDA/Machine-Learning/blob/main/12.png?raw=true'>\n",
    "\n",
    "Convolution layer works by sliding a window $W$, say kernel or filter, and multiplying pixel values and filter values point-wisely.\n",
    "\n",
    "Then we get locally summarized information for the shape of data.\n",
    "\n",
    "Following equation tells us the output size.\n",
    "\n",
    "<img width = '1000' height = '600' src = 'https://github.com/SHlee-TDA/Machine-Learning/blob/main/13.png?raw=true'>\n",
    "<img width = '1000' height = '600' src = 'https://github.com/SHlee-TDA/Machine-Learning/blob/main/14.png?raw=true'>\n",
    "\n",
    "- $C_{in}$ : input channel. Our MNIST image has only one channel, but in general image has RGB 3-channels.\n",
    "- $H_{in}, W_{in}$ : input height and input width.\n",
    "- $C_{out}$ : output channel. We can choose this parameter and it determines how many number of features are generated by our convolution layer.\n",
    "- $padding$ : literally, pad some numbers (in general 0) on input image to control the output size.\n",
    "- $stride$ : a unit of each sliding window step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation : Convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2667, -0.2667, -0.2667,  ..., -0.2667, -0.2667, -0.2667],\n",
       "          [-0.2667, -0.2667, -0.2667,  ..., -0.2667, -0.2667, -0.2667],\n",
       "          [-0.2667, -0.2667, -0.2667,  ..., -0.2667, -0.2667, -0.2667],\n",
       "          ...,\n",
       "          [-0.2667, -0.2667, -0.2667,  ..., -0.2667, -0.2667, -0.2667],\n",
       "          [-0.2667, -0.2667, -0.2667,  ..., -0.2667, -0.2667, -0.2667],\n",
       "          [-0.2667, -0.2667, -0.2667,  ..., -0.2667, -0.2667, -0.2667]],\n",
       "\n",
       "         [[-0.2272, -0.2272, -0.2272,  ..., -0.2272, -0.2272, -0.2272],\n",
       "          [-0.2272, -0.2272, -0.2272,  ..., -0.2272, -0.2272, -0.2272],\n",
       "          [-0.2272, -0.2272, -0.2272,  ..., -0.2272, -0.2272, -0.2272],\n",
       "          ...,\n",
       "          [-0.2272, -0.2272, -0.2272,  ..., -0.2272, -0.2272, -0.2272],\n",
       "          [-0.2272, -0.2272, -0.2272,  ..., -0.2272, -0.2272, -0.2272],\n",
       "          [-0.2272, -0.2272, -0.2272,  ..., -0.2272, -0.2272, -0.2272]],\n",
       "\n",
       "         [[-0.0974, -0.0974, -0.0974,  ..., -0.0974, -0.0974, -0.0974],\n",
       "          [-0.0974, -0.0974, -0.0974,  ..., -0.0974, -0.0974, -0.0974],\n",
       "          [-0.0974, -0.0974, -0.0974,  ..., -0.0974, -0.0974, -0.0974],\n",
       "          ...,\n",
       "          [-0.0974, -0.0974, -0.0974,  ..., -0.0974, -0.0974, -0.0974],\n",
       "          [-0.0974, -0.0974, -0.0974,  ..., -0.0974, -0.0974, -0.0974],\n",
       "          [-0.0974, -0.0974, -0.0974,  ..., -0.0974, -0.0974, -0.0974]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3240, -0.3240, -0.3240,  ..., -0.3240, -0.3240, -0.3240],\n",
       "          [-0.3240, -0.3240, -0.3240,  ..., -0.3240, -0.3240, -0.3240],\n",
       "          [-0.3240, -0.3240, -0.3240,  ..., -0.3240, -0.3240, -0.3240],\n",
       "          ...,\n",
       "          [-0.3240, -0.3240, -0.3240,  ..., -0.3240, -0.3240, -0.3240],\n",
       "          [-0.3240, -0.3240, -0.3240,  ..., -0.3240, -0.3240, -0.3240],\n",
       "          [-0.3240, -0.3240, -0.3240,  ..., -0.3240, -0.3240, -0.3240]],\n",
       "\n",
       "         [[-0.1853, -0.1853, -0.1853,  ..., -0.1853, -0.1853, -0.1853],\n",
       "          [-0.1853, -0.1853, -0.1853,  ..., -0.1853, -0.1853, -0.1853],\n",
       "          [-0.1853, -0.1853, -0.1853,  ..., -0.1853, -0.1853, -0.1853],\n",
       "          ...,\n",
       "          [-0.1853, -0.1853, -0.1853,  ..., -0.1853, -0.1853, -0.1853],\n",
       "          [-0.1853, -0.1853, -0.1853,  ..., -0.1853, -0.1853, -0.1853],\n",
       "          [-0.1853, -0.1853, -0.1853,  ..., -0.1853, -0.1853, -0.1853]],\n",
       "\n",
       "         [[ 0.2226,  0.2226,  0.2226,  ...,  0.2226,  0.2226,  0.2226],\n",
       "          [ 0.2226,  0.2226,  0.2226,  ...,  0.2226,  0.2226,  0.2226],\n",
       "          [ 0.2226,  0.2226,  0.2226,  ...,  0.2226,  0.2226,  0.2226],\n",
       "          ...,\n",
       "          [ 0.2226,  0.2226,  0.2226,  ...,  0.2226,  0.2226,  0.2226],\n",
       "          [ 0.2226,  0.2226,  0.2226,  ...,  0.2226,  0.2226,  0.2226],\n",
       "          [ 0.2226,  0.2226,  0.2226,  ...,  0.2226,  0.2226,  0.2226]]]],\n",
       "       grad_fn=<ThnnConv2DBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## One can make a convolution layer simply.\n",
    "\n",
    "conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "conv1(sample_img[None, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 28, 28])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Knowing the output size of convolution layer is important.\n",
    "\n",
    "conv1(sample_img[None, :, :, :]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.2226, 0.2226, 0.2226,  ..., 0.2226, 0.2226, 0.2226],\n",
       "          [0.2226, 0.2226, 0.2226,  ..., 0.2226, 0.2226, 0.2226],\n",
       "          [0.2226, 0.2226, 0.2226,  ..., 0.2226, 0.2226, 0.2226],\n",
       "          ...,\n",
       "          [0.2226, 0.2226, 0.2226,  ..., 0.2226, 0.2226, 0.2226],\n",
       "          [0.2226, 0.2226, 0.2226,  ..., 0.2226, 0.2226, 0.2226],\n",
       "          [0.2226, 0.2226, 0.2226,  ..., 0.2226, 0.2226, 0.2226]]]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## In general, ReLU layer is placed after convolution layer.\n",
    "\n",
    "relu = nn.ReLU()\n",
    "layer = nn.Sequential(conv1, relu)\n",
    "\n",
    "layer(sample_img[None, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 14, 14])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Finally, Maxpooling layer is placed after.\n",
    "## Maxpooling layer works sub-sampling to reduce the output size and then to leave important features.\n",
    "\n",
    "pool = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
    "layer = nn.Sequential(conv1, relu, pool)\n",
    "\n",
    "# This is a typical convolution layer structure.\n",
    "# Note that the output size of convolution layer.\n",
    "layer(sample_img[None, :,:,:]).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Network Structure\n",
    "\n",
    "<img width = '1000' height = '600' src = 'https://github.com/SHlee-TDA/Machine-Learning/blob/main/15.png?raw=true'>\n",
    "\n",
    "\n",
    "Typical CNN structure consists of several convolution layers in front and fully connected layers (MLPs) at the end.\n",
    "\n",
    "CNN structure can make more powerful performance, because it uses the locally geometric shape of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation : CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model (2 conv layers)\n",
    "class CNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # L1 ImgIn shape=(?, 1, 28, 28)\n",
    "        #    Conv     -> (?, 32, 28, 28)\n",
    "        #    Pool     -> (?, 32, 14, 14)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "\n",
    "        # L2 ImgIn shape=(?, 32, 14, 14\n",
    "        #    Conv      ->(?, 64, 14, 14)\n",
    "        #    Pool      ->(?, 64, 7, 7)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # Final FC 7x7x64 inputs -> 10 outputs\n",
    "\n",
    "        \n",
    "        self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(x.size(0), -1)   # Flatten them for FC\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0257,  0.0603, -0.0232,  0.0068,  0.0359,  0.0069,  0.0655,  0.0449,\n",
       "          0.0431, -0.0574]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN()\n",
    "model(sample_img[None,:,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample_img[None, :, :,:]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Softmax is internally computed.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started. It takes sometime.\n",
      "[Epoch:    1] cost = 0.558016539\n",
      "[Epoch:    2] cost = 0.125402912\n",
      "[Epoch:    3] cost = 0.0824157596\n",
      "[Epoch:    4] cost = 0.0646317825\n",
      "[Epoch:    5] cost = 0.0545987934\n",
      "[Epoch:    6] cost = 0.0472714379\n",
      "[Epoch:    7] cost = 0.0421830155\n",
      "[Epoch:    8] cost = 0.0372184142\n",
      "[Epoch:    9] cost = 0.0340796709\n",
      "[Epoch:   10] cost = 0.030795373\n",
      "[Epoch:   11] cost = 0.0307574291\n",
      "[Epoch:   12] cost = 0.0269902181\n",
      "[Epoch:   13] cost = 0.0242694262\n",
      "[Epoch:   14] cost = 0.022371849\n",
      "[Epoch:   15] cost = 0.0210635904\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# train my model\n",
    "total_batch = len(data_loader)\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        # image is already size of (28x28), no reshape\n",
    "        # label is not one-hot encoded\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        \n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample_img[None, :, :, :]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leeseongheon/opt/anaconda3/envs/TDAwithCNN/lib/python3.8/site-packages/torchvision/datasets/mnist.py:57: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.984000027179718\n"
     ]
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "\n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you!\n",
    "\n",
    "Writer : Seong heon, Lee (POSTECH MINDS)\n",
    "Contact : shlee0125@postech.ac.kr\n",
    "Github : https://github.com/SHlee-TDA\n",
    "MINDS Github : https://github.com/postech-minds/postech-minds\n",
    "\n",
    "# References \n",
    "\n",
    "1. Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow - Aurelien, Geron\n",
    "2. 모두를 위한 딥러닝 시즌 2 -파이토치- [Deeplearningzerotoall](https://deeplearningzerotoall.github.io/season2/)\n",
    "3. HiddenBeginner(이동진) blog posts (https://hiddenbeginner.github.io/)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c77395e1ed64a909e0735f657d47dd38afff6577797d59de5d393769af0253af"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
